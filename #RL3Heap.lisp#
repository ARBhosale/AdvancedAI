;; This is the modilfied version for 3 Heap Nim
;; A modified version of the assignment Reinforcement Learning
;; Look for the original file for provided instructions



(defun random-elt (sequence)
  "Returns a random element from a sequence"
  (elt sequence (random (length sequence))))

(defun num-states (q-table)
  "Returns the number of states in a q-table"
  (first (array-dimensions q-table)))

(defun num-heaps (q-table &optional state)
  "Returns the number of actions in a q-table"
  (second (array-dimensions q-table)))

(defun num-actions (q-table &optional state)
  "Returns the number of actions in a q-table"
  (third (array-dimensions q-table)))



(defun make-q-table (num-states num-actions num-heaps)
  "Makes a q-table, with initial values all set to 0"
  (make-array (list num-states num-heaps  num-actions) :initial-element 0))

(defun max-q (q-table state)
  (let* ((num-actions (num-heaps (num-actions q-table)))
	 (best (aref q-table state (1- num-actions))))  ;; q of last action
    (dotimes (action (1- num-actions) best)  ;; all but last action...
      (setf best (max (aref q-table state action) best)))))

(defun max-action (q-table state &optional val)
  ;; a little inefficient, but what the heck...
  (let ((num-actions (num-actions q-table))
	(best (max-q q-table state))
	bag)
    (dotimes (action num-actions)
      (when (= (aref q-table state action) best)
	(push action bag)))
    (if (and val (rest bag))
	val
      (random-elt bag))))

(defparameter *basic-alpha* 0.5 "A simple alpha constant")
(defun basic-alpha (iteration)
  (declare (ignore iteration)) ;; quiets compiler complaints
  *basic-alpha*)

(defun get-probability (new-state-index old-state-index action-index)
  (if (= new-state-index (+ old-state-index (+ action-index 1)))
    0.8 0.2))

(defun get-reward (current-state-index heap-size)
  (cond ((= (1+ current-

state-index) (1- heap-size)) -1)
    ((= (1+ current-state-index) heap-size) 1)
    (t 0)))

(defun get-future-utility (current-state-index action-index q-table)
;; using Algorithm number 123
  (let* ((future-utility 0))
    (setf state-index current-state-index)
    (dotimes (state-index (num-states q-table))
      (setf future-utility (+ future-utility
        (* (get-probability state-index current-state-index action-index)
          (max-q q-table state-index)))))))

(defun q-learner (q-table reward current-state action next-state gamma alpha-func iteration)

  ;;; IMPLEMENT ME


  (let (
	 (alpha (funcall alpha-func iteration))
        )

    

;    (print q-table)
    
     
	  
    (setf (aref q-table current-state action)
	  (+ (* (- 1 alpha) (aref q-table current-state action))
	     (* alpha (+ reward (* gamma (max-q q-table next-state))))
	  )
    )
    
    
    q-table
  )
 

)

(defun ask-if-user-goes-first ()
  "Returns true if the user wants to go first"
  (y-or-n-p "Do you want to play first?"))

(defun make-user-move ()
  "Returns the number of sticks the user wants to remove"
  (let ((result))
    (loop
     (format t "~%Take how many sticks?  ")
     (setf result (read))
     (when (and (numberp result) (<= result 3) (>= result 1))
       (return result))
     (format t "~%Answer must be between 1 and 3"))))

(defun play-nim (q-table heap-size)
  "Plays a game of nim.  Asks if the user wants to play first,
then has the user play back and forth with the game until one of
them wins.  Reports the winner."

  ;;; IMPLEMENT ME

  (let (player
	(state 0)


	) 

    (if (ask-if-user-goes-first)
	(setf player 0) ;user
	(setf player 1) ;computer
    )
    
    (loop

       (format t "Current State : ~S~%"state)

       
       (if (eq player 0)
	   (let ((user-action (make-user-move)))

	     (setf state (+ state (+ 0 user-action)))
	     
	     (setf player 1)
	     )
	   (let ((computer-action (max-action q-table state)))

	     (format t "Computer Played ~S~%"(+ 1 computer-action))
	     (setf state (+ state (+ 1 computer-action)))	     
	     
	     (setf player 0)
	     )
       )


       

       (when (>= state heap-size)
	 (if (eq player 1)
	     (print "You Lose")
	     (print "You Win")
	 )
	 (return)
       )
    )


  )
)


(defun best-actions (q-table)
  "Returns a list of the best actions.  If there is no best action, this is indicated with a hyphen (-)"
  ;; hint: see optional value in max-action function

  ;;; IMPLEMENT ME

  (let ((output-list '())
	max-act
	)

    (dotimes (i (num-states q-table))

      (setf max-act (max-action q-table i -1))

      (if (<= max-act 0)
	  (setf output-list (append output-list (list '-)))
	  (setf output-list (append output-list (list max-act)))
	  
      )
	     
	     
     )
    

    output-list
   )
   
  )





(defun learn-nim (heap-size gamma alpha-func num-iterations)
  "Returns a q-table after learning how to play nim"
  ;;; IMPLEMENT ME
  
  (let* ((state 0)
	old-state

	(q-table (make-q-table (+ heap-size 6) 3))
	my-move
	reward
	opponents-move 
	
	 )

    (loop for iteration from 1 to num-iterations do

	 (setf state 0)

	 (loop while (< state heap-size) do
;	      (print state)
	      (setf old-state state)
	      (setf my-move (max-action q-table state))

	      (setf state (+ state (+ 1 my-move)))
	      
	      (if (>= state heap-size)
		  (setf reward -1)
		  (progn
		    (setf opponents-move (max-action q-table state)) 
		    (setf state (+ state (+ opponents-move 1)))

		    (if (>= state heap-size)
			(setf reward 1)
			)

		    
		  )
		  )

	      
	      (if (and (not (eq reward 1)) (not (eq reward -1)))
		  (setf reward 0)
		  )
	      
	      (setf q-table (q-learner q-table reward old-state my-move state gamma alpha-func iteration))


	 )

     )

    q-table
    
  )


  
  
  
)

;(learn-nim 22 0.1 #'basic-alpha 50000)
;(print (learn-nim 22 0.1 #'basic-alpha 50000))
;; sbcl
;; example:
;; 
;(setq *my-q-table* (learn-nim 22 0.1 #'basic-alpha 50000))

;(print *my-q-table*)
;;
;; to get the policy from this table:
;;
; (print (best-actions *my-q-table*))
;;
;; to play a game of your brain versus this q-table:
;;
; (play-nim *my-q-table* 22)   ;; need to provide the original heap size
;;
;; You might try changing to some other function than #'basic-alpha...



;; Now the q-table states will be a combination of the 3 heaps
;; The possible actions will be 9 -> 1-3 on each heap
